import os
import datetime
import random
import string
import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import time

# 1. æº–å‚™
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
today = datetime.date.today()
slug = ''.join(random.choices(string.ascii_lowercase + string.digits, k=15))
filename = f"articles/{slug}.md"

# 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ï¼ˆæ”¹å–„ç‰ˆï¼‰
def fetch_article_content(url, max_length=2000):
    """
    Webãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'
        }
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’15ç§’ã«å»¶é•·
        response = requests.get(url, timeout=15, headers=headers)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä¸è¦ãªã‚¿ã‚°ã‚’å‰Šé™¤
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # è¨˜äº‹æœ¬æ–‡ã®å–å¾—ï¼ˆarticleã‚¿ã‚°å„ªå…ˆï¼‰
        article = soup.find('article')
        if article:
            text_parts = [p.get_text(strip=True) for p in article.find_all('p')]
        else:
            text_parts = [p.get_text(strip=True) for p in soup.find_all('p')]
        
        full_text = " ".join(filter(None, text_parts))
        
        # æ–‡å­—æ•°åˆ¶é™
        return full_text[:max_length] if full_text else "ï¼ˆæœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼‰"
        
    except requests.exceptions.Timeout:
        return "ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚µã‚¤ãƒˆã®å¿œç­”ãŒé…ã„ãŸã‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰"
    except requests.exceptions.RequestException as e:
        return f"ï¼ˆã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}ï¼‰"
    except Exception as e:
        return f"ï¼ˆäºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}ï¼‰"

# 3. ãƒã‚¿å…ƒå–å¾—
rss_url = "https://hnrss.org/best?count=10" 
print(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­: {rss_url}")
feed = feedparser.parse(rss_url)

if not feed.entries:
    raise Exception("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# è¨˜äº‹æ•°ã‚’5æœ¬ã«å‰Šæ¸›ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰
target_entries = feed.entries[:5]
print(f"{len(target_entries)}æœ¬ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¾ã™...")

articles_data = ""
for i, entry in enumerate(target_entries, 1):
    print(f"[{i}/{len(target_entries)}] Reading: {entry.title[:60]}...")
    content_text = fetch_article_content(entry.link, max_length=1500)
    
    articles_data += f"""
ã€è¨˜äº‹{i}ã€‘
Source Title: {entry.title}
Source URL: {entry.link}
Source Content (æŠœç²‹): {content_text}
----
"""
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: 1ç§’å¾…æ©Ÿ
    time.sleep(1)

print("AIãŒè§£èª¬è¨˜äº‹ã‚’åŸ·ç­†ä¸­...")

# 4. AIã¸ã®æŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ï¼‰
system_prompt = """
ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘æƒ…å ±ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
æ¸¡ã•ã‚ŒãŸã€Œæµ·å¤–ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ¬æ–‡ã€ã‚’èª­ã¿ã€Zennèª­è€…å‘ã‘ã«æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ï¼šURLã®æ‰±ã„ã€‘
å…¥åŠ›ã•ã‚ŒãŸã€ŒSource URLã€ã¯ã€**çµ¶å¯¾ã«æ”¹å¤‰ã›ãšã€ãã®ã¾ã¾å‡ºåŠ›ã«å«ã‚ã¦ãã ã•ã„ã€‚**

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## [æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«]
[Source URLã‚’ãã®ã¾ã¾è»¢è¨˜]

**ã©ã‚“ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼Ÿ:**
(è¨˜äº‹ã®ä¸­èº«ã‚’å…ƒã«ã€ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã®ã‹ã€ä½•ãŒèµ·ããŸã®ã‹ã‚’å…·ä½“çš„ã«2-3è¡Œã§)

**ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¸ã®å½±éŸ¿:**
(é–‹ç™ºè€…ã«ã¨ã£ã¦ã©ã†ã„ã†ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã‹ã€æŠ€è¡“çš„èƒŒæ™¯ã‚’å«ã‚ã¦ç°¡æ½”ã«è§£èª¬)

---
(ã“ã‚Œã‚’å…¨è¨˜äº‹åˆ†ç¹°ã‚Šè¿”ã™)

ã€æ³¨æ„äº‹é …ã€‘
- æœ¬æ–‡ãŒå–å¾—ã§ããªã‹ã£ãŸè¨˜äº‹ã¯ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã®ã¿æ²è¼‰
- æ¨æ¸¬ã‚„æ†¶æ¸¬ã¯é¿ã‘ã€æœ¬æ–‡ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’è¦ç´„
"""

user_prompt = f"""
ä»¥ä¸‹ã®è‹±èªè¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ã€æ—¥æœ¬ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚
æ—¥ä»˜: {today}

{articles_data}
"""

# 5. AIå®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
try:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.2,
        max_tokens=3000, # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›
    )
    ai_text = response.choices[0].message.content
    
    # ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å‡ºåŠ›
    usage = response.usage
    print(f"ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {usage.total_tokens} tokens")
    print(f"æ¨å®šã‚³ã‚¹ãƒˆ: ${usage.total_tokens * 0.00000015:.6f}")
    
except Exception as e:
    raise Exception(f"OpenAI APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

# 6. ä¿å­˜
full_content = f"""---
title: "ã€Hacker Newsã€‘æµ·å¤–ãƒˆãƒ¬ãƒ³ãƒ‰é€Ÿå ± ({today})"
emoji: "ğŸ“°"
type: "tech"
topics: ["news", "technology", "hackernews"]
published: false
---

ä¸–ç•Œä¸­ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒæ³¨ç›®ã—ã¦ã„ã‚‹ã€ŒHacker Newsã€ã®è©±é¡Œè¨˜äº‹ãƒˆãƒƒãƒ—{len(target_entries)}ã‚’ã€AIãŒä¸­èº«ã‚’èª­ã‚“ã§è§£èª¬ã—ã¾ã™ã€‚

{ai_text}

---
â€»ã“ã®è¨˜äº‹ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆãƒ»è¦ç´„ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºãªæƒ…å ±ã¯å…ƒè¨˜äº‹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
"""

os.makedirs("articles", exist_ok=True)
with open(filename, "w", encoding="utf-8") as f:
    f.write(full_content)

print(f"âœ… åŸ·ç­†å®Œäº†ï¼: {filename}")
