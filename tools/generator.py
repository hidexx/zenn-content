import os
import datetime
import random
import string
import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

# 1. æº–å‚™
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
today = datetime.date.today()
slug = ''.join(random.choices(string.ascii_lowercase + string.digits, k=15))
filename = f"articles/{slug}.md"

# 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ï¼ˆWebãƒšãƒ¼ã‚¸ã®ä¸­èº«ã‚’èª­ã‚€æ©Ÿèƒ½ï¼‰
def fetch_article_content(url):
    try:
        # 5ç§’å¾…ã£ã¦ãƒ€ãƒ¡ãªã‚‰è«¦ã‚ã‚‹è¨­å®š
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        
        # HTMLã‹ã‚‰æ–‡å­—ã ã‘æŠœãå‡ºã™
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # <p>ã‚¿ã‚°ï¼ˆæœ¬æ–‡ï¼‰ã‚’ä¸­å¿ƒã«å–å¾—
        text_parts = [p.get_text() for p in soup.find_all('p')]
        full_text = " ".join(text_parts)
        
        # é•·ã™ãã‚‹ã¨AIãŒãƒ‘ãƒ³ã‚¯ã™ã‚‹ã®ã§ã€å…ˆé ­2000æ–‡å­—ã ã‘è¿”ã™
        return full_text[:2000]
    except Exception as e:
        return "ï¼ˆæœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬ã—ã¾ã™ï¼‰"

# 3. ãƒã‚¿å…ƒå–å¾—
rss_url = "https://hnrss.org/best?count=10" 
feed = feedparser.parse(rss_url)

articles_data = ""
# è¨˜äº‹æ•°ï¼ˆ7æœ¬ï¼‰
target_entries = feed.entries[:7]

print(f"{len(target_entries)}æœ¬ã®è¨˜äº‹ã®ä¸­èº«ã‚’èª­ã¿ã«è¡Œãã¾ã™...")

for i, entry in enumerate(target_entries):
    print(f"Reading: {entry.title}...")
    # ã“ã“ã§ä¸­èº«ã‚’èª­ã¿ã«è¡Œãï¼
    content_text = fetch_article_content(entry.link)
    
    articles_data += f"""
    ã€è¨˜äº‹{i+1}ã€‘
    Source Title: {entry.title}
    Source URL: {entry.link}
    Source Content (æŠœç²‹): {content_text}
    ----
    """

print("AIãŒè§£èª¬è¨˜äº‹ã‚’åŸ·ç­†ä¸­...")

# 4. AIã¸ã®æŒ‡ç¤º
system_prompt = """
ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘æƒ…å ±ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
æ¸¡ã•ã‚ŒãŸã€Œæµ·å¤–ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ¬æ–‡ã€ã‚’èª­ã¿ã€Zennèª­è€…å‘ã‘ã«æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ï¼šURLã®æ‰±ã„ã€‘
å…¥åŠ›ã•ã‚ŒãŸã€ŒSource URLã€ã¯ã€**çµ¶å¯¾ã«æ”¹å¤‰ã›ãšã€ãã®ã¾ã¾å‡ºåŠ›ã«å«ã‚ã¦ãã ã•ã„ã€‚**

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## [æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«]
[Source URLã‚’ãã®ã¾ã¾è»¢è¨˜]

**ã©ã‚“ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼Ÿ:**
(è¨˜äº‹ã®ä¸­èº«ã‚’å…ƒã«ã€ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã®ã‹ã€ä½•ãŒèµ·ããŸã®ã‹ã‚’å…·ä½“çš„ã«3è¡Œã§)

**ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¸ã®å½±éŸ¿:**
(é–‹ç™ºè€…ã«ã¨ã£ã¦ã©ã†ã„ã†ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã‹ã€æŠ€è¡“çš„èƒŒæ™¯ã‚’å«ã‚ã¦è§£èª¬)

---
(ã“ã‚Œã‚’å…¨è¨˜äº‹åˆ†ç¹°ã‚Šè¿”ã™)
"""

user_prompt = f"""
ä»¥ä¸‹ã®è‹±èªè¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ã€æ—¥æœ¬ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚
æ—¥ä»˜: {today}

{articles_data}
"""

# 5. AIå®Ÿè¡Œ
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.2,
    max_tokens=4000, # è¨˜äº‹æ•°ãŒå¤šã„ã®ã§æ ã‚’åºƒã’ã‚‹
)
ai_text = response.choices[0].message.content

# 6. ä¿å­˜
full_content = f"""---
title: "ã€Hacker Newsã€‘æµ·å¤–ãƒˆãƒ¬ãƒ³ãƒ‰é€Ÿå ± ({today})"
emoji: "ğŸ“°"
type: "tech"
topics: ["news", "technology", "hackernews"]
published: false
---

ä¸–ç•Œä¸­ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒæ³¨ç›®ã—ã¦ã„ã‚‹ã€ŒHacker Newsã€ã®è©±é¡Œè¨˜äº‹ãƒˆãƒƒãƒ—7ã‚’ã€AIãŒä¸­èº«ã‚’èª­ã‚“ã§è§£èª¬ã—ã¾ã™ã€‚

{ai_text}

---
â€»ã“ã®è¨˜äº‹ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆãƒ»è¦ç´„ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºãªæƒ…å ±ã¯å…ƒè¨˜äº‹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
"""

os.makedirs("articles", exist_ok=True)
with open(filename, "w", encoding="utf-8") as f:
    f.write(full_content)

print(f"åŸ·ç­†å®Œäº†ï¼: {filename}")
